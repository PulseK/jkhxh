
# 数据源配置
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.datasource.url=jdbc:mysql://172.16.1.101:3306/spring?characterEncoding=utf8&useSSL=true
spring.datasource.username=root
spring.datasource.password=Jkdz8888

#mybatis
#entity扫描的包名
mybatis.type-aliases-package=com.haxiha.goods.domain
#Mapper.xml所在的位置
mybatis.mapper-locations=classpath*:com/haxiha/goods/dao/*.xml 

#redis集群配置
spring.redis.cluster.nodes=172.16.1.153:6386,172.16.1.153:6385,172.16.1.152:6383,172.16.1.152:6384,172.16.1.154:6388,172.16.1.154:6387,172.16.1.151:6382,172.16.1.151:6381
## 连接池最大连接数（使用负值表示没有限制）
spring.redis.pool.max-active=300
## Redis数据库索引(默认为0)
spring.redis.database=0
## 连接池最大阻塞等待时间（使用负值表示没有限制）
spring.redis.pool.max-wait=-1
## 连接池中的最大空闲连接
spring.redis.pool.max-idle=100
## 连接池中的最小空闲连接
spring.redis.pool.min-idle=20
## 连接超时时间（毫秒）
spring.redis.timeout=60000


#pagehelper分页插件配置
pagehelper.helperDialect=mysql
pagehelper.reasonable=true
pagehelper.supportMethodsArguments=true
pagehelper.params= count=countSql

#eureka
eureka.instance.preferIpAddress=true
eureka.instance.instance-id=${spring.cloud.client.ipAddress}:${server.port}:${spring.application.name}

#kafka
#brokers集群
spring.kafka.bootstrap-servers=172.16.1.244:9092 
#即所有副本都同步到数据时send方法才返回, 以此来完全判断数据是否发送成功, 理论上来讲数据不会丢失.
sspring.kafka.custom-producer.acks=all
#发送失败重试次数
spring.kafka.custom-producer.retries=5 
#批处理条数：当多个记录被发送到同一个分区时，生产者会尝试将记录合并到更少的请求中。这有助于客户端和服务器的性能。
spring.kafka.custom-producer.batch-size=1638
#批处理延迟时间上限：即1ms过后，不管是否达到批处理数，都直接发送一次请求 
spring.kafka.custom-producer.linger-ms=1 
#即32MB的批处理缓冲区
spring.kafka.custom-producer.buffer-memory=33554432 
spring.kafka.custom-producer.request-timeout-ms=10000
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#消费者群组ID，发布-订阅模式，即如果一个生产者，多个消费者都要消费，那么需要定义自己的群组，同一群组内的消费者只有一个能消费到消息
spring.kafka.custom-consumer.group-id=goodsGroup  
#如果为true，消费者的偏移量将在后台定期提交。
spring.kafka.custom-consumer.enable-auto-commit=false
spring.kafka.custom-consumer.auto-offset-reset=earliest
spring.kafka.custom-consumer.max-poll-records=1000
spring.kafka.custom-consumer.key-deserializerr=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.custom-consumer.value-deserializer=org.apache.kafka.common.serialization.StringSerializer

